{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data'에 복제합니다...\n",
      "remote: Enumerating objects: 36, done.\u001b[K\n",
      "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 36 (delta 9), reused 26 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
      "오브젝트를 받는 중: 100% (36/36), 483.12 KiB | 26.00 KiB/s, 완료.\n",
      "델타를 알아내는 중: 100% (9/9), 완료.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!git clone https://github.com/taehojo/data.git\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 14:02:52.523548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "41/41 [==============================] - 0s 608us/step - loss: 0.1450 - accuracy: 0.9523\n",
      "Test accuracy: 0.9523077011108398\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, 0:12]\n",
    "y = df.iloc[:, 12]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0)\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {score[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "modelpath = './data/model/{epoch:02d}-{val_accuracy:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=0)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=500,\n",
    "                    validation_split=0.25, verbose=0, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAG2CAYAAACXuTmvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAANWVJREFUeJzt3Ql4FFW6xvEvZGVLIkQTCWFRkN0ge4IaURSUEXBFhhkQUcaLAgKiwGVxh6sDogRB7owyOiIRlUVEhkXAhSBLYJRFtkFgGHYkCTsmdZ/vcLtJIIGQ9JLk/H/PU3aq+nR1dRHpl3O+UxXgOI4jAAAAFinj7wMAAADwNQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALCO3wPQxIkTpUaNGhIWFiYtW7aUlStX5tt2w4YN8sADD5j2AQEBMn78+CLvEwAA2MevASglJUUGDhwoo0aNkrS0NImPj5d27drJgQMH8mx/4sQJue6662TMmDESExPjkX0CAAD7BPjzZqjaO9O8eXNJTk4269nZ2RIXFyd9+/aVIUOGXPK12sPzzDPPmMVT+wQAAHYI8tcbnzlzRtasWSNDhw51bytTpoy0bdtWUlNTfbrP06dPm8VFQ9ORI0ekcuXKZqgNAAAUf9qnk5mZKVWqVDHf/8UyAB06dEiysrIkOjo613Zd//nnn326z9GjR8uLL75YqPcEAADFy+7du6Vq1arFMwAVJ9pjpHVDLunp6VKtWjVzAsPDw/16bACQl3nzRLp21V5u7bUW+fhjkXvu8eMBaYnBu++eOxg9qCef1H9dFr5dqTxJ8LaMjAxT9lKxYsXLtvVbAIqKipLAwEDZv39/ru26nl+Bs7f2GRoaapYLafghAAEojn74QSQwUCQr69yjTnZ95BE/HlD79iKTJp0/qHbt9C/RwrcrlScJvlKQ8hW/zQILCQmRpk2byuLFi3PV3uh6QkJCsdknABRHbdqc/17Xx9tu8/MBdewoMnu2SL9+5x51vSjtSuVJQnHi1yEwHXbq0aOHNGvWTFq0aGGu63P8+HHp2bOneb579+4SGxtranRcRc4bN250/7xnzx5Zt26dVKhQQWrVqlWgfQJAaeDKEUuXnvtezy9HzJkjsmTJuSzgzazhPqiCvElB2/nqJMFKfp0Gr3S6+htvvCH79u2Txo0by9tvv22msqvbbrvNTHefOnWqWf/ll1+kZs2aF+0jKSlJluoveAH2WdAxxIiICFMLxBAYgJJKw0+nTuc7QLzd4VLq+TRNojCu5Pvb7wGoOCIAASgNBgwQmTDh/CiQjjqNGyfFhs7aPXv2bN5Pfv31uRoe/cfr7bf7+tDyPp4+fc6nyXfeKR7HZZng4GBT6+uJ729mgQFAKaUdFXrHoMuVwPi6Y0P/3a099EePHs27wYkT52ZuuWo3N2wQKVcu73anTomEheX9vCdFRIhMnnx+XWcZ7djh3fdEniIjI83EpqJep48ABAClVEFKYHIOk2lYym+YzJMhyRV+rrnmGilXrtzFX2R792pKOr9eqZLItdfmbpORIXL8uE7jPde2cmXvzSZTuv9du86vV6vm3ffzJNe5Kl++5BxzPsFZb4nlurXVtRf+TlwhAhAAlGKXqzfWUJNzpriGpQvbFzQkFXTYyxV+9Gr7ebrqKpHDh3Ovay9PTgcP5l7Xq/lf2MaTdN8hISKZmed6fyIjxau0d8wT76X7cQU3Pac6Ycjbx+5FZcuWNY8agvR36FLDYcX+bvAAAP8pyEzxvEJSXjQoad2RPuZHa3505Cojo5z5bs6TfkHrF7Ve1T+/L+wLL3RXgAvfFZkeR1ycb8LPtm16Ebtzj/meqALQEHWp9RJIew1VvvVjBUQAAgCLFeSyPAUJSa5eIi261sf8QpDWEmvnzeHDAZf+br9c2ChISCqpPBla/BEUvcxT9+hkCAwALHe5YbKC1BIVZChN6cSunNel1e/2QmcXfWFpCj45Q0rOOxoUJbS4gqKvhu5KEAIQAKDIIamgM84uvCRbseiQ8FS9jad4OrSU1qBYRAyBAQCKrKB3uNBL51x99blJVcVi5KoA9TY65HKp5YUXXsi9v927C1y3o6+fNWvWZYcAly1bJrfffrtUqlTJ1MDUrl3b3PVA74pQUHphYb07gqfalXT0AAEAPKKgd7jQGladwezNSVtFqre5IJXt1Wn5/y8lJUVGjhwpmzdvdm/T2zHlClNKA5WHEp7eAqp9+/bSt29fc2cDnQm1detW+eyzz8ysOhQOPUAAAHsVoEhYL7rnWvQqw9prk3Pb9OnTpV69ehIWEyN1H3xQ3pkx49wLMzNND83TTz9trlkTFhYm1atXd9/fUnta1H333Wf26Vq/0IIFC8z7vP7669KwYUO5/vrrTSD63//9X/e0cPXdd9/JLbfcYrbFxcVJv379zL0wXbeW2rlzpwwYMMDdc1VYkyZNMsegNyCvU6eOfPjhh7mu1aM9YtWqVZPQ0FCpUqWKOQ6Xd955x/Re6bmIjo6WBx98UPyFAAQAKHYKNJJUkHn3l1PE2WQfffSR6RF69dVXZdMPP8hrffrIiHfflb/NnWvClPbYzJkzRz755BPTa6TtXUFn1apV5vH99983vUyu9Qtp+NHnv/nmm3yPY/v27SYUPfDAA/Ljjz+anioNRBq+1Oeffy5Vq1aVl156yexrb45erSsxc+ZM6d+/vwwaNEjWr18vf/rTn8zNxpdoFbyI6ZV688035d133zW9VDq816hRI/Pc6tWrTRjSY9BzMX/+fLn11lvFb/ReYMgtPT1dL0FqHgEAnnPy5Eln48aN5jE/v/7qOKtWnV90/SKzZ+v1n53swEDzaNZ94P3333ciIiLc69dff70zbdq0XAf/8rPPOgnNm5vVvn37OrfffruTnZ2dq42za5d51O+amTNnXvI9f/vtN+fRRx81bWNiYpzOnTs7EyZMyPUd1atXL6d37965Xvftt986ZcqUcfbuPWneLi6uuvPmm29e9jNWr55/u8TEROeJJ57Ite2hhx5y7rnnHvPz2LFjnRtuuME5c+bMRa/97LPPnPDwcCcjI8Px1u/QlXx/0wMEAChxl8E5NX+JOIGBEpCVZR5P/SOfqzN6kQ4vac9Lr169TB2QWapWlVcmTJDtO3eaNo8++qisW7fODBVp78eCzz/PXXRdAHq1Y+0l+ve//22GwWJjY+W1116TBg0auHty/vnPf8rUqVPPH0eFCtKuXTvJzs6Wb7/dYd7ut99ETp4s2mfetGmTtG7dOtc2Xdft6qGHHpKTJ0/KddddJ0888YTpMfpN31hE7rzzTjMEqM/98Y9/NL1hemsLfyEAAQCKlYJcuy+zaRt3+NHHzCb5zLv3omPHjplHrcXRkONadGhoxYoV5rkmTZrIjh075OWXXzbB4OGePeXB558v1Ptp8NHgkJycLBs2bJBTp07J5P+/Qaseiw5H5TwODUXffLNVqla93r0PvXesN8XFxZnhLa310VqkPn36mGEuvWpzxYoVJS0tTT7++GNTE6VDh/Hx8fnfFNfLmAUGAChWCnIZnOAHOsrWo7OlYtpSE36ufqCjzy8DpEW8WuT7r3/9S7p163bRvlzCw8OlS5cuZnmwfXtp/+CDciQ9XSpFREhwcHChZnJdddVVJkS4ipw1aOlssVp64nKIijrf0RQcHCLBwUWbNVavXj35/vvvzRR8F12vX7++e12Dz7333muWp556SurWrSs//fSTOcagoCBp27atWUaNGmXu7P7111/L/fffL75GAAIAFDuXu3afea5XR8l8uKNcfYngcrlwU9SZ6y+++KIZ2tLZYYmJ7WXbttOyceNqycz8VUaNGijvvTfOBJWbbrpJypQpIzO++kpioqMlUt8oIsIURC9evNgMI+msKQ02F9KCYu3R0dliOvtKe34++OAD0ws0Qe89IiLPP/+8tGrVyhQ9P/7441K+fHkTiBYuXCivvJJszsH119eQVau+kT17HjHvFaXpKB979uwx75mTDl8NHjxYHn74YfN5NMR88cUXpsB60aJFps3Ud96RrOPHpWXr1lIuJkb+/ve/m0Ckr507d64Ji9ojpJ9z3rx5ZohOhwf9okiVSKUURdAA4L8iaE8pSDG1FgfnbKPrV1IErT766COncePGTkhIiBMefpVz0023Oq+//rnZ15QpU8xz5cuXNwXAd9xxh5OWluZ+7Zw5c5xatWo5QUFBpvg4L9r+D3/4g1OzZk0nNDTUqVy5snPrrbea1+a0cuVK584773QqVKhg3u/GG290Xn31VffzqampZpvuQy78+s9RmK3Hoc9fuHw4ebJp886f/+xcd911TnBwsCl4/uCDD9z7mPnGG07Lhg2d8PLlzTG0atXKWbRokbsoOykpybnqqqucsmXLmmNJSUlx/FUEHaD/8U/0Kr4yMjJMmk9PTzddlwAAz9DeC62JqVmzprkWjDfpNPqct9TSme56ceX8eoBUUa5d6Ml9FfgNPXG7jKMFOPCCtCnICffy79CVfH9TBA0AsLaY2pM3lffpDeoLcAsPj067yyxAmxJ253lqgAAApVJB7ynqyXuF+uy+o5mZclQiJVMqSkXJlMg8buHh0bvPVyxAmxJ253kCEACg1CqtN0I/WuYq2SZ6DzJH9ku01CpzTAr9MSMj5ei1dSUz3ZGKEQESGVmheKRJLyMAAQDgw5IbT8jMPhd+RPSeXo5ZjyzKaNrec6Fn/wmRWuXz+XwlKNwUBDVAAAD4sOSmQPc5u4xzI1CuG5oGFKncJrMA5T2lEQEIAAAfhQRPBSlPFlxXLFm1yx7DEBgAAJdRkBrgwgapwoYXT41IRZas2mWPIQABAOCjkOCpIOVpkaWrvKdACEAAAPgoJBQ0SBWnguvSihogAAB8SAONXiC5ceMaMn78eK8WXCN/BCAAAC4hICDgkssLL7xQqP2uWrVKevfuXaSCa70lxO9//3tzV3q9LUTVqlWlU6dO8vPPPxf4OB599FHp3Lmzx9r5ZBqcBzAEBgDAJezdu9f9c0pKiowcOVI2b97s3lahwvkLB+rtNbOysiQo6PJfr1dffXWR6oTOnj0rd955p7mbut6RXe86/+9//1u++uorOVpcu42O5rinmH5Ir98zJH/0AAEAcAkxMTHuRW+0qb0+rnXtaalYsaIJHU2bNpXQ0FD57rvvZPv27aYnJjo62gSk5s2by6JFi3Ltt0aN3ENgut+//OUv0rPnfXLrreXkoYdqy5Ytc/LNBxs2bDDv884770irVq2kevXq0rp1a3nllVfMusvu3bvl4YcflsjISKlUqZI5rl9++cU8p71Xf/vb32T27NnuHq2lS5cW6jwtW7ZMWrRoYc6BhrEhQ4bIb7/95n7+008/lUatWknZm2+Wym3bSts+feT4/yc9fU99bfny5c1x6ufYuXOneBMBCABQIs2ZIzJgwLlHf9Mv+zFjxsimTZvkxhtvlGPHjsk999wjixcvlrVr10r79u3l3nvvlV27dl1yPy+++KIJKz/++KN07HiP/OlP3eTIkSP59iCVKVPGBAvtdcqvl6hdu3YmpH377bfy/fffm0DWvn17OXPmjDz77LPm/XRde7p0SUxMzHNfZ86InDyZ98jVnj17zOfVoPfPf/5TJk2aJH/9619NGFO6365du8pj3bvLpk8+kaWTJ8v9bdqIU768CUk6tJaUlGQ+d2pqqhka1DDmVQ4ukp6ertcXN48AAM85efKks3HjRvNYFLNnO45+gwUGnnvUdV94//33nYiICPf6kiVLzPfFrFmzLvvaBg0aOBMmTHCvV69e3XnzzTfd67qf4cOHu9ePHTtmtn311Vf57jM5OdkpV66cU7FiRadNmzbOSy+95Gzfvt39/IcffujUqVPHyc7Odm87ffq0U7ZsWecf//iHWe/Ro4fTqVOnSx77r786TocOPZykpE7OqlXn1nMaNmzYRe8zceJEp0KFCk5WVpazZs0a81l++eWXcy/etcu9k8OHD5vnli5d6hT1d+hKvr/pAQIAlDhLlogEBopox4c+FnLUxmOaNWuWa117gLR3pV69emZIR3tdtHfocj1A2nvkosNB4eHhcuDAgXzbP/XUU7Jv3z756KOPJCEhQWbMmCENGjSQhQsXmue1N2bbtm2mB0iPQRcdBjt16pQZPiuoyxVm62fT909PD3DXN+swlp4HrUuKj4+XO+64Qxo1aiQPPfGE/O/8+fKryXxijkcLrLWnSnvJ3nrrrVx1V95CAAIAlDht2pwPP/p4223+PR4NKzlp+Jk5c6a89tprZuhp3bp15stfh50uJTg4ONe6DgNlZ2df8jUabjQ4vPrqqybw3HLLLe6hJw0gWpuk759z2bJli5k95snbZehHyzl9PyPj/HOBgYEmlGmtVP369WXChAmmeFtnsan333/fDH3p8JsWmt9www2yYsUK8SYCEACgxOnYUWT2bJF+/c496npxorU22qtx3333meCjBdOuwmNv0sBUt25dOX78uFlv0qSJbN26Va655hqpVatWriUiIsK0CQkJybeGyEULscPDRUJD8564pT1dP/yQambBuXzzzfcmnOnUfNexaa+Q1jlpXZS+r4ZEl5tuukmGDh0qy5cvl4YNG8q0adPEmwhAAIASSUPPuHHFL/yo2rVrm6np2tuivTLa23K5npwrpfvWGV1aBL1x40Yz1KWFx++9957Zrrp16yZRUVFmXXuitMdFZ1z169fPDE25ZqNp8bFO7T906JApnM5LSIj28qTLL7/k7k3SWWZ9+vSRvXt3yxtv9JVffvlZli2bLW++OUoGDhxoCrV/+OEH0xu2evVqMwyo5+bgwYMmOOkxafDRHiCd+bVgwQIT2vQ5b+I6QAAAeNi4cePkscceM0M6GkCef/55ycg5JuQB2rOi4UV7VLR3SXtYXOsDdHqciJQrV06++eYb8/7333+/ZGZmSmxsrKnH0foi9cQTT5hQpHVMOmS2ZMkSuS2fMUVtpz01OfXq1ctM3583b54MHDhYunWLl6uuqiSPP95Lhg8fbtroe+lx6LR/PQ+xsdXl5ZfHyt133y379+83lxPQ6fiHDx82U+i1tulPf/qTeFOAVkJ79R1KIP3D0a7B9PR09y8IAKDotPhW/8Vfs2ZNc+Vi2OVojusgqsJcB/FSv0NX8v3NEBgAAPCJK7nNh7cRgAAAgE8UZDaZr1ADBAAAfEKHu3TYS3t+NPz46TZgBgEIAAD4jIYefwYfF4bAAAA+x/wb+Pt3hwAEAPAZ15WOT5w44e9DQQnl+t258KrZV4ohMACAz+gtEfTeWK77W+l1arx+12+Ump6fEydOmN8d/R3S36WiIAABAHxKbwuhLnWTTyA/Gn5cv0NFQQACAPiU9vjo1X71/lT53XYByIsOexW158eFAAQA8Av9IvPUlxlwpSiCBgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALCO3wPQxIkTpUaNGhIWFiYtW7aUlStXXrL9jBkzpG7duqZ9o0aNZN68ebmeP3bsmDz99NNStWpVKVu2rNSvX18mT57s5U8BAABKEr8GoJSUFBk4cKCMGjVK0tLSJD4+Xtq1aycHDhzIs/3y5cula9eu0qtXL1m7dq107tzZLOvXr3e30f3Nnz9f/v73v8umTZvkmWeeMYFozpw5PvxkAACgOAtwHMfx15trj0/z5s0lOTnZrGdnZ0tcXJz07dtXhgwZclH7Ll26yPHjx2Xu3Lnuba1atZLGjRu7e3kaNmxo2o0YMcLdpmnTpnL33XfLK6+8UqDjysjIkIiICElPT5fw8HAPfFIAAOBtV/L97bceoDNnzsiaNWukbdu25w+mTBmznpqamudrdHvO9kp7jHK2T0xMNL09e/bsEc12S5YskS1btshdd92V77GcPn3anLScCwAAKL38FoAOHTokWVlZEh0dnWu7ru/bty/P1+j2y7WfMGGCqfvRGqCQkBBp3769qTO69dZb8z2W0aNHm8ToWrQXCgAAlF5+L4L2NA1AK1asML1A2sM0duxYeeqpp2TRokX5vmbo0KGmu8y17N6926fHDAAAfCtI/CQqKkoCAwNl//79ubbrekxMTJ6v0e2Xan/y5EkZNmyYzJw5Uzp06GC23XjjjbJu3Tr585//fNHwmUtoaKhZAACAHfzWA6TDU1qcvHjxYvc2LYLW9YSEhDxfo9tztlcLFy50tz979qxZtJYoJw1aum8AAAC/9gC5pqz36NFDmjVrJi1atJDx48ebWV49e/Y0z3fv3l1iY2NNjY7q37+/JCUlmWEt7eGZPn26rF69WqZMmWKe14pvfX7w4MHmGkDVq1eXZcuWyQcffCDjxo3z50cFAADFiF8DkE5XP3jwoIwcOdIUMut0dr2Gj6vQedeuXbl6c3SG17Rp02T48OFmqKt27doya9YsM/XdRUOR1vR069ZNjhw5YkLQq6++Kk8++aRfPiMAACh+/HodoOKK6wABAFDylIjrAAEAAPgLAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOn4PQBMnTpQaNWpIWFiYtGzZUlauXHnJ9jNmzJC6deua9o0aNZJ58+Zd1GbTpk3SsWNHiYiIkPLly0vz5s1l165dXvwUAACgJPFrAEpJSZGBAwfKqFGjJC0tTeLj46Vdu3Zy4MCBPNsvX75cunbtKr169ZK1a9dK586dzbJ+/Xp3m+3bt8vNN99sQtLSpUvlxx9/lBEjRpjABAAAoAIcx3H8dSq0x0d7Z5KTk816dna2xMXFSd++fWXIkCEXte/SpYscP35c5s6d697WqlUrady4sUyePNmsP/LIIxIcHCwffvhhoY8rIyPD9B6lp6dLeHh4ofcDAAB850q+v/3WA3TmzBlZs2aNtG3b9vzBlClj1lNTU/N8jW7P2V5pj5GrvQaoL7/8Um644Qaz/ZprrjEha9asWZc8ltOnT5uTlnMBAACll98C0KFDhyQrK0uio6Nzbdf1ffv25fka3X6p9jp0duzYMRkzZoy0b99eFixYIPfdd5/cf//9smzZsnyPZfTo0SYxuhbthQIAAKWX34ugPUl7gFSnTp1kwIABZmhMh9J+97vfuYfI8jJ06FDTXeZadu/e7cOjBgAAvhYkfhIVFSWBgYGyf//+XNt1PSYmJs/X6PZLtdd9BgUFSf369XO1qVevnnz33Xf5HktoaKhZAACAHfzWAxQSEiJNmzaVxYsX5+rB0fWEhIQ8X6Pbc7ZXCxcudLfXfWpR9ebNm3O12bJli1SvXt0rnwMAAJQ8fusBUjoFvkePHtKsWTNp0aKFjB8/3szy6tmzp3m+e/fuEhsba2p0VP/+/SUpKUnGjh0rHTp0kOnTp8vq1atlypQp7n0OHjzYzBa79dZbpU2bNjJ//nz54osvzJR4AAAAvwcgDSoHDx6UkSNHmkJmrdnRwOIqdNaLF+rMMJfExESZNm2aDB8+XIYNGya1a9c2M7waNmzobqNFz1rvo6GpX79+UqdOHfnss8/MtYEAAAD8fh2g4orrAAEAUPKUiOsAAQAA+AsBCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsUKgD97W9/ky+//NK9/txzz0lkZKQkJibKzp07PXl8AAAAxSMAvfbaa1K2bFnzc2pqqkycOFFef/11iYqKkgEDBnj6GAEAADwqqDAv2r17t9SqVcv8PGvWLHnggQekd+/e0rp1a7nttts8e4QAAADFoQeoQoUKcvjwYfPzggUL5M477zQ/h4WFycmTJz17hAAAAMWhB0gDz+OPPy433XSTbNmyRe655x6zfcOGDVKjRg1PHyMAAID/e4C05ichIUEOHjwon332mVSuXNlsX7NmjXTt2tWzRwgAAOBhAY7jOJ7eaUmXkZEhERERkp6eLuHh4f4+HAAA4OHv70L1AM2fP1++++67XD1CjRs3lt///vfy66+/FmaXAAAAPlOoADR48GCTstRPP/0kgwYNMnVAO3bskIEDB3r6GAEAAPxfBK1Bp379+uZnrQH63e9+Z64NlJaW5i6IBgAAKFU9QCEhIXLixAnz86JFi+Suu+4yP1eqVMndMwQAAFCqeoBuvvlmM9SlFz5cuXKlpKSkmO06Jb5q1aqePkYAAAD/9wAlJydLUFCQfPrppzJp0iSJjY0127/66itp3769Z48QAADAw5gGnwemwQMAULq/vws1BKaysrLMfcA2bdpk1hs0aCAdO3aUwMDAwu4SAADAJwoVgLZt22Zme+3Zs0fq1Kljto0ePVri4uLkyy+/lOuvv97TxwkAAODfGqB+/fqZkKN3hdep77rs2rVLatasaZ4DAAAodT1Ay5YtkxUrVphp7y56P7AxY8aYmWEAAAClrgcoNDRUMjMzL9p+7Ngxc40gAACAUheA9MrPvXv3lh9++EF0Epku2iP05JNPmkJoAACAUheA3n77bVMDlJCQIGFhYWZJTEyUWrVqyfjx4z1/lAAAAP6uAYqMjJTZs2eb2WCuafD16tUzAQgAAKDUBKDL3eV9yZIl7p/HjRtXtKMCAAAoDgFo7dq1BWoXEBBQlOMBAAAoPgEoZw8PAACAdUXQAAAAJRkBCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALBOsQhAEydOlBo1akhYWJi0bNlSVq5cecn2M2bMkLp165r2jRo1knnz5uXb9sknn5SAgAAZP368F44cAACURH4PQCkpKTJw4EAZNWqUpKWlSXx8vLRr104OHDiQZ/vly5dL165dpVevXrJ27Vrp3LmzWdavX39R25kzZ8qKFSukSpUqPvgkAACgpPB7ABo3bpw88cQT0rNnT6lfv75MnjxZypUrJ++9916e7d966y1p3769DB48WOrVqycvv/yyNGnSRJKTk3O127Nnj/Tt21c++ugjCQ4O9tGnAQAAJYFfA9CZM2dkzZo10rZt2/MHVKaMWU9NTc3zNbo9Z3ulPUY522dnZ8sf//hHE5IaNGhw2eM4ffq0ZGRk5FoAAEDp5dcAdOjQIcnKypLo6Ohc23V93759eb5Gt1+u/f/8z/9IUFCQ9OvXr0DHMXr0aImIiHAvcXFxhfo8AACgZPD7EJinaY+SDpNNnTrVFD8XxNChQyU9Pd297N692+vHCQAALA1AUVFREhgYKPv378+1XddjYmLyfI1uv1T7b7/91hRQV6tWzfQC6bJz504ZNGiQmWmWl9DQUAkPD8+1AACA0suvASgkJESaNm0qixcvzlW/o+sJCQl5vka352yvFi5c6G6vtT8//vijrFu3zr3oLDCtB/rHP/7h5U8EAABKgiB/H4BOge/Ro4c0a9ZMWrRoYa7Xc/z4cTMrTHXv3l1iY2NNnY7q37+/JCUlydixY6VDhw4yffp0Wb16tUyZMsU8X7lyZbPkpLPAtIeoTp06fviEAACguPF7AOrSpYscPHhQRo4caQqZGzduLPPnz3cXOu/atcvMDHNJTEyUadOmyfDhw2XYsGFSu3ZtmTVrljRs2NCPnwIAAJQkAY7jOP4+iOJGp8HrbDAtiKYeCACA0vf9XepmgQEAAFwOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOsUiAE2cOFFq1KghYWFh0rJlS1m5cuUl28+YMUPq1q1r2jdq1EjmzZvnfu7s2bPy/PPPm+3ly5eXKlWqSPfu3eU///mPDz4JAAAoCfwegFJSUmTgwIEyatQoSUtLk/j4eGnXrp0cOHAgz/bLly+Xrl27Sq9evWTt2rXSuXNns6xfv948f+LECbOfESNGmMfPP/9cNm/eLB07dvTxJwMAAMVVgOM4jj8PQHt8mjdvLsnJyWY9Oztb4uLipG/fvjJkyJCL2nfp0kWOHz8uc+fOdW9r1aqVNG7cWCZPnpzne6xatUpatGghO3fulGrVql32mDIyMiQiIkLS09MlPDy8SJ8PAAD4xpV8f/u1B+jMmTOyZs0aadu27fkDKlPGrKempub5Gt2es73SHqP82is9EQEBARIZGZnn86dPnzYnLecCAABKL78GoEOHDklWVpZER0fn2q7r+/bty/M1uv1K2p86dcrUBOmwWX5pcPTo0SYxuhbtgQIAAKWX32uAvEkLoh9++GHRUb5Jkybl227o0KGml8i17N6926fHCQAAfCtI/CgqKkoCAwNl//79ubbrekxMTJ6v0e0Fae8KP1r38/XXX19yLDA0NNQsAADADn7tAQoJCZGmTZvK4sWL3du0CFrXExIS8nyNbs/ZXi1cuDBXe1f42bp1qyxatEgqV67sxU8BAABKGr/2ACmdAt+jRw9p1qyZmak1fvx4M8urZ8+e5nm9hk9sbKyp01H9+/eXpKQkGTt2rHTo0EGmT58uq1evlilTprjDz4MPPmimwOtMMa0xctUHVapUyYQuAABgN78HIJ3WfvDgQRk5cqQJKjqdff78+e5C5127dpmZYS6JiYkybdo0GT58uAwbNkxq164ts2bNkoYNG5rn9+zZI3PmzDE/675yWrJkidx2220+/XwAAKD48ft1gIojrgMEAEDJU2KuAwQAAOAPBCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xSLADRx4kSpUaOGhIWFScuWLWXlypWXbD9jxgypW7euad+oUSOZN29erucdx5GRI0fKtddeK2XLlpW2bdvK1q1bvfwpAABASeH3AJSSkiIDBw6UUaNGSVpamsTHx0u7du3kwIEDebZfvny5dO3aVXr16iVr166Vzp07m2X9+vXuNq+//rq8/fbbMnnyZPnhhx+kfPnyZp+nTp3y4ScDAADFVYCj3SV+pD0+zZs3l+TkZLOenZ0tcXFx0rdvXxkyZMhF7bt06SLHjx+XuXPnure1atVKGjdubAKPfpwqVarIoEGD5NlnnzXPp6enS3R0tEydOlUeeeSRyx5TRkaGREREmNeFh4d79PMCAADvuJLv7yDxozNnzsiaNWtk6NCh7m1lypQxQ1apqal5vka3a49RTtq7M2vWLPPzjh07ZN++fWYfLnoyNGjpa/MKQKdPnzaLi54414kEAAAlg+t7uyB9O34NQIcOHZKsrCzTO5OTrv/88895vkbDTV7tdbvrede2/NpcaPTo0fLiiy9etF17ogAAQMmSmZlpOj+KbQAqLrQHKmevkg7DHTlyRCpXriwBAQEeT6carHbv3s3wmg9wvn2L8+1bnG/f4nwX//OtPT8afrQU5nL8GoCioqIkMDBQ9u/fn2u7rsfExOT5Gt1+qfauR92ms8ByttE6obyEhoaaJafIyEjxJv3D5H8g3+F8+xbn27c4377F+S7e5/tyPT/FYhZYSEiING3aVBYvXpyr90XXExIS8nyNbs/ZXi1cuNDdvmbNmiYE5WyjKVJng+W3TwAAYBe/D4Hp0FOPHj2kWbNm0qJFCxk/fryZ5dWzZ0/zfPfu3SU2NtbU6aj+/ftLUlKSjB07Vjp06CDTp0+X1atXy5QpU8zzOmT1zDPPyCuvvCK1a9c2gWjEiBGmO0ynywMAAPg9AOm09oMHD5oLF2qRsg5TzZ8/313EvGvXLjMzzCUxMVGmTZsmw4cPl2HDhpmQozPAGjZs6G7z3HPPmRDVu3dvOXr0qNx8881mn3rhRH/ToTa95tGFQ27wDs63b3G+fYvz7Vuc79J1vv1+HSAAAADrrgQNAADgawQgAABgHQIQAACwDgEIAABYhwDkQxMnTpQaNWqY2Wh6b7KVK1f6+5BKhW+++Ubuvfdec6kDvQyC675wLlrnr7MM9cKYZcuWNfeJ27p1q9+Ot6TTS1LoDYwrVqwo11xzjbm8xObNm3O1OXXqlDz11FPmauoVKlSQBx544KILmKJgJk2aJDfeeKP7YnB6PbOvvvrK/Tzn2rvGjBnjvryKC+fcc1544QVzfnMudevW9cm5JgD5SEpKirnmkU7pS0tLk/j4eHMT1wMHDvj70Eo8veSBnk8NmHl5/fXX5e2335bJkyebC2KWL1/enHv9HwtXbtmyZeYvpBUrVpiLkJ49e1buuusu8+fgMmDAAPniiy9kxowZpv1//vMfuf/++/163CVV1apVzZew3jhar3l2++23S6dOnWTDhg3mec6196xatUreffddE0Bz4px7VoMGDWTv3r3u5bvvvvPNudZp8PC+Fi1aOE899ZR7PSsry6lSpYozevRovx5XaaO/0jNnznSvZ2dnOzExMc4bb7zh3nb06FEnNDTU+fjjj/10lKXLgQMHzHlftmyZ+/wGBwc7M2bMcLfZtGmTaZOamurHIy09rrrqKucvf/kL59qLMjMzndq1azsLFy50kpKSnP79+5vtnHPPGjVqlBMfH5/nc94+1/QA+cCZM2fMv9506MVFL+6o66mpqX49ttJux44d5gKbOc+93idGhyA5956Rnp5uHitVqmQe9Xdde4VynnPt0q5WrRrnvIiysrLM1e+1t02HwjjX3qO9nHq3gZznVnHOPU9LErSE4brrrpNu3bqZCyD74lz7/UrQNjh06JD5i8t1dWsXXf/555/9dlw20PCj8jr3rudQeHrvPq2NaN26tftq7Hpe9T5/F95QmHNeeD/99JMJPDpsq3UQM2fOlPr168u6des4116gIVNLFXQI7EL8fnuW/mN06tSpUqdOHTP89eKLL8ott9wi69ev9/q5JgABKNK/kvUvqpxj9vA8/XLQsKO9bZ9++qm5f6LWQ8Dzdu/ebe45qfVtxeH2SaXd3Xff7f5Za600EFWvXl0++eQTM2nFmxgC84GoqCgJDAy8qHJd1/XO9fAe1/nl3Hve008/LXPnzpUlS5aYQl0XPa867Kv34cuJc154+q/gWrVqSdOmTc0sPC36f+uttzjXXqDDLjo5pUmTJhIUFGQWDZs6kUJ/1t4Hzrn3aG/PDTfcINu2bfP67zcByEd/eelfXIsXL841dKDr2q0N76lZs6b5HyXnuc/IyDCzwTj3haO15hp+dBjm66+/Nuc4J/1dDw4OznXOdZq8jutzzj1D//44ffo059oL7rjjDjPkqD1urqVZs2amNsX1M+fce44dOybbt283ly3x+u93kcuoUSDTp083M4+mTp3qbNy40endu7cTGRnp7Nu3z9+HVipma6xdu9Ys+is9btw48/POnTvN82PGjDHnevbs2c6PP/7odOrUyalZs6Zz8uRJfx96ifRf//VfTkREhLN06VJn79697uXEiRPuNk8++aRTrVo15+uvv3ZWr17tJCQkmAVXbsiQIWaG3Y4dO8zvr64HBAQ4CxYsMM9zrr0v5ywwxTn3nEGDBpm/S/T3+/vvv3fatm3rREVFmdml3j7XBCAfmjBhgvmDDAkJMdPiV6xY4e9DKhWWLFligs+FS48ePdxT4UeMGOFER0ebEHrHHXc4mzdv9vdhl1h5nWtd3n//fXcbDZd9+vQx07XLlSvn3HfffSYk4co99thjTvXq1c3fG1dffbX5/XWFH8W59n0A4px7TpcuXZxrr73W/H7Hxsaa9W3btvnkXAfof4rejwQAAFByUAMEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgACmDp0qUSEBBw0X2JAJRMBCAAAGAdAhAAALAOAQhAibkD+ujRo83d58uWLSvx8fHy6aef5hqe+vLLL+XGG2+UsLAwadWqlaxfvz7XPj777DNp0KCBhIaGSo0aNWTs2LG5ntc7rD///PMSFxdn2tSqVUv++te/5mqzZs0ac0fwcuXKSWJiork7NYCShwAEoETQ8PPBBx/I5MmTZcOGDTJgwAD5wx/+IMuWLXO3GTx4sAk1q1atkquvvlruvfdeOXv2rDu4PPzww/LII4/ITz/9JC+88IKMGDFCpk6d6n599+7d5eOPP5a3335bNm3aJO+++65UqFAh13H893//t3mP1atXS1BQkDz22GM+PAsAPIWboQIo9rRnplKlSrJo0SJJSEhwb3/88cflxIkT0rt3b2nTpo1Mnz5dunTpYp47cuSIVK1a1QQcDT7dunWTgwcPyoIFC9yvf+6550yvkQaqLVu2SJ06dWThwoXStm3bi45Be5n0PfQY7rjjDrNt3rx50qFDBzl58qTpdQJQctADBKDY27Ztmwk6d955p+mRcS3aI7R9+3Z3u5zhSAOTBhrtyVH62Lp161z71fWtW7dKVlaWrFu3TgIDAyUpKemSx6JDbC7XXnuteTxw4IDHPisA3wjy0fsAQKEdO3bMPGpvTWxsbK7ntFYnZwgqLK0rKojg4GD3z1p35KpPAlCy0AMEoNirX7++CTq7du0yhck5Fy1YdlmxYoX7519//dUMa9WrV8+s6+P333+fa7+6fsMNN5ien0aNGpkgk7OmCEDpRQ8QgGKvYsWK8uyzz5rCZw0pN998s6Snp5sAEx4eLtWrVzftXnrpJalcubJER0ebYuWoqCjp3LmzeW7QoEHSvHlzefnll02dUGpqqiQnJ8s777xjntdZYT169DBFzVoErbPMdu7caYa3tIYIQOlCAAJQImhw0ZldOhvsX//6l0RGRkqTJk1k2LBh7iGoMWPGSP/+/U1dT+PGjeWLL76QkJAQ85y2/eSTT2TkyJFmX1q/o4Hp0Ucfdb/HpEmTzP769Okjhw8flmrVqpl1AKUPs8AAlHiuGVo67KXBCAAuhxogAABgHQIQAACwDkNgAADAOvQAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAACxzf8BUcVs4eEppHkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df\n",
    "\n",
    "v_loss = hist_df['val_loss']\n",
    "t_loss = hist_df['loss']\n",
    "x_len = np.arange(len(t_loss))\n",
    "\n",
    "plt.plot(x_len, v_loss, 'o', c='red', markersize=2, label='Test Set Loss')\n",
    "plt.plot(x_len, t_loss, 'o', c='blue', markersize=2, label='Train Set Loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0.0, 0.10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0886 - accuracy: 0.9731 - val_loss: 0.0942 - val_accuracy: 0.9715\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0896 - accuracy: 0.9725 - val_loss: 0.0917 - val_accuracy: 0.9723\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0907 - accuracy: 0.9700 - val_loss: 0.0894 - val_accuracy: 0.9700\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0908 - accuracy: 0.9720 - val_loss: 0.0979 - val_accuracy: 0.9592\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0865 - accuracy: 0.9746 - val_loss: 0.0883 - val_accuracy: 0.9677\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9746 - val_loss: 0.0894 - val_accuracy: 0.9715\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0858 - accuracy: 0.9749 - val_loss: 0.0867 - val_accuracy: 0.9708\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0844 - accuracy: 0.9761 - val_loss: 0.0864 - val_accuracy: 0.9692\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0832 - accuracy: 0.9764 - val_loss: 0.0858 - val_accuracy: 0.9692\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0820 - accuracy: 0.9777 - val_loss: 0.0852 - val_accuracy: 0.9692\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9782 - val_loss: 0.0871 - val_accuracy: 0.9646\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0820 - accuracy: 0.9751 - val_loss: 0.0849 - val_accuracy: 0.9715\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0818 - accuracy: 0.9779 - val_loss: 0.0846 - val_accuracy: 0.9715\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0804 - accuracy: 0.9779 - val_loss: 0.0834 - val_accuracy: 0.9708\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0801 - accuracy: 0.9777 - val_loss: 0.0829 - val_accuracy: 0.9715\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0795 - accuracy: 0.9777 - val_loss: 0.0826 - val_accuracy: 0.9700\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0794 - accuracy: 0.9784 - val_loss: 0.0823 - val_accuracy: 0.9715\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0789 - accuracy: 0.9779 - val_loss: 0.0824 - val_accuracy: 0.9700\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9769 - val_loss: 0.0837 - val_accuracy: 0.9685\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0787 - accuracy: 0.9772 - val_loss: 0.0813 - val_accuracy: 0.9723\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9764 - val_loss: 0.0830 - val_accuracy: 0.9738\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0783 - accuracy: 0.9787 - val_loss: 0.0809 - val_accuracy: 0.9723\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.9774 - val_loss: 0.0815 - val_accuracy: 0.9746\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0780 - accuracy: 0.9784 - val_loss: 0.0798 - val_accuracy: 0.9708\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0758 - accuracy: 0.9795 - val_loss: 0.0790 - val_accuracy: 0.9723\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0757 - accuracy: 0.9795 - val_loss: 0.0793 - val_accuracy: 0.9700\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9784 - val_loss: 0.0783 - val_accuracy: 0.9723\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9792 - val_loss: 0.0808 - val_accuracy: 0.9700\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9792 - val_loss: 0.0795 - val_accuracy: 0.9762\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0771 - accuracy: 0.9784 - val_loss: 0.0782 - val_accuracy: 0.9738\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0761 - accuracy: 0.9782 - val_loss: 0.0779 - val_accuracy: 0.9731\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0749 - accuracy: 0.9772 - val_loss: 0.0807 - val_accuracy: 0.9708\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0748 - accuracy: 0.9797 - val_loss: 0.0766 - val_accuracy: 0.9738\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0731 - accuracy: 0.9795 - val_loss: 0.0760 - val_accuracy: 0.9738\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0726 - accuracy: 0.9797 - val_loss: 0.0788 - val_accuracy: 0.9715\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0739 - accuracy: 0.9782 - val_loss: 0.0753 - val_accuracy: 0.9754\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0720 - accuracy: 0.9802 - val_loss: 0.0753 - val_accuracy: 0.9754\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0718 - accuracy: 0.9797 - val_loss: 0.0750 - val_accuracy: 0.9746\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0717 - accuracy: 0.9792 - val_loss: 0.0784 - val_accuracy: 0.9723\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0723 - accuracy: 0.9795 - val_loss: 0.0745 - val_accuracy: 0.9738\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0710 - accuracy: 0.9802 - val_loss: 0.0745 - val_accuracy: 0.9762\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.9792 - val_loss: 0.0764 - val_accuracy: 0.9731\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9772 - val_loss: 0.0749 - val_accuracy: 0.9746\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9759 - val_loss: 0.0796 - val_accuracy: 0.9738\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9784 - val_loss: 0.0793 - val_accuracy: 0.9738\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0728 - accuracy: 0.9792 - val_loss: 0.0729 - val_accuracy: 0.9754\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0708 - accuracy: 0.9800 - val_loss: 0.0726 - val_accuracy: 0.9762\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0711 - accuracy: 0.9795 - val_loss: 0.0748 - val_accuracy: 0.9738\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0703 - accuracy: 0.9792 - val_loss: 0.0767 - val_accuracy: 0.9731\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0731 - accuracy: 0.9766 - val_loss: 0.0715 - val_accuracy: 0.9785\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0716 - accuracy: 0.9787 - val_loss: 0.0713 - val_accuracy: 0.9777\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9800 - val_loss: 0.0744 - val_accuracy: 0.9769\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.9808 - val_loss: 0.0723 - val_accuracy: 0.9754\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9797 - val_loss: 0.0732 - val_accuracy: 0.9746\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9800 - val_loss: 0.0800 - val_accuracy: 0.9715\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0715 - accuracy: 0.9779 - val_loss: 0.0704 - val_accuracy: 0.9785\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9815 - val_loss: 0.0735 - val_accuracy: 0.9777\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0673 - accuracy: 0.9815 - val_loss: 0.0698 - val_accuracy: 0.9785\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0669 - accuracy: 0.9797 - val_loss: 0.0723 - val_accuracy: 0.9746\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0664 - accuracy: 0.9813 - val_loss: 0.0715 - val_accuracy: 0.9785\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0671 - accuracy: 0.9792 - val_loss: 0.0694 - val_accuracy: 0.9800\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.9802 - val_loss: 0.0716 - val_accuracy: 0.9746\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.9810 - val_loss: 0.0710 - val_accuracy: 0.9746\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0675 - accuracy: 0.9805 - val_loss: 0.0686 - val_accuracy: 0.9792\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0665 - accuracy: 0.9805 - val_loss: 0.0684 - val_accuracy: 0.9792\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9808 - val_loss: 0.0683 - val_accuracy: 0.9785\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0653 - accuracy: 0.9802 - val_loss: 0.0687 - val_accuracy: 0.9800\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0663 - accuracy: 0.9800 - val_loss: 0.0675 - val_accuracy: 0.9792\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.9795 - val_loss: 0.0685 - val_accuracy: 0.9808\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.9808 - val_loss: 0.0695 - val_accuracy: 0.9769\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.9808 - val_loss: 0.0692 - val_accuracy: 0.9785\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9820 - val_loss: 0.0681 - val_accuracy: 0.9762\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0643 - accuracy: 0.9800 - val_loss: 0.0671 - val_accuracy: 0.9800\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.9802 - val_loss: 0.0678 - val_accuracy: 0.9785\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0638 - accuracy: 0.9802 - val_loss: 0.0666 - val_accuracy: 0.9792\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9800 - val_loss: 0.0671 - val_accuracy: 0.9800\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 0.9805 - val_loss: 0.0661 - val_accuracy: 0.9785\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9808 - val_loss: 0.0683 - val_accuracy: 0.9754\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0647 - accuracy: 0.9805 - val_loss: 0.0673 - val_accuracy: 0.9762\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.9826 - val_loss: 0.0661 - val_accuracy: 0.9800\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0635 - accuracy: 0.9800 - val_loss: 0.0656 - val_accuracy: 0.9792\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.9797 - val_loss: 0.0657 - val_accuracy: 0.9800\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.9805 - val_loss: 0.0661 - val_accuracy: 0.9792\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0625 - accuracy: 0.9813 - val_loss: 0.0658 - val_accuracy: 0.9792\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0629 - accuracy: 0.9813 - val_loss: 0.0649 - val_accuracy: 0.9800\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.9808 - val_loss: 0.0650 - val_accuracy: 0.9800\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.9808 - val_loss: 0.0667 - val_accuracy: 0.9808\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.9800 - val_loss: 0.0659 - val_accuracy: 0.9800\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 0.9815 - val_loss: 0.0645 - val_accuracy: 0.9800\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.9810 - val_loss: 0.0643 - val_accuracy: 0.9800\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0617 - accuracy: 0.9810 - val_loss: 0.0641 - val_accuracy: 0.9808\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.9815 - val_loss: 0.0663 - val_accuracy: 0.9769\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0619 - accuracy: 0.9805 - val_loss: 0.0652 - val_accuracy: 0.9777\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9823 - val_loss: 0.0680 - val_accuracy: 0.9792\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9810 - val_loss: 0.0670 - val_accuracy: 0.9792\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0653 - accuracy: 0.9826 - val_loss: 0.0636 - val_accuracy: 0.9800\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.9815 - val_loss: 0.0636 - val_accuracy: 0.9808\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9813 - val_loss: 0.0662 - val_accuracy: 0.9762\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0620 - accuracy: 0.9810 - val_loss: 0.0636 - val_accuracy: 0.9808\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 0.9810 - val_loss: 0.0638 - val_accuracy: 0.9800\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 0.9818 - val_loss: 0.0624 - val_accuracy: 0.9808\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9810 - val_loss: 0.0647 - val_accuracy: 0.9785\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.9820 - val_loss: 0.0630 - val_accuracy: 0.9800\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.9813 - val_loss: 0.0637 - val_accuracy: 0.9792\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 0.9818 - val_loss: 0.0627 - val_accuracy: 0.9815\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0592 - accuracy: 0.9815 - val_loss: 0.0620 - val_accuracy: 0.9815\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0595 - accuracy: 0.9823 - val_loss: 0.0617 - val_accuracy: 0.9823\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9838 - val_loss: 0.0633 - val_accuracy: 0.9792\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.9805 - val_loss: 0.0621 - val_accuracy: 0.9800\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0585 - accuracy: 0.9831 - val_loss: 0.0616 - val_accuracy: 0.9808\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.9813 - val_loss: 0.0625 - val_accuracy: 0.9808\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0584 - accuracy: 0.9815 - val_loss: 0.0631 - val_accuracy: 0.9792\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0582 - accuracy: 0.9836 - val_loss: 0.0613 - val_accuracy: 0.9815\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.9808 - val_loss: 0.0657 - val_accuracy: 0.9815\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9800 - val_loss: 0.0703 - val_accuracy: 0.9800\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0643 - accuracy: 0.9800 - val_loss: 0.0615 - val_accuracy: 0.9808\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.9823 - val_loss: 0.0679 - val_accuracy: 0.9738\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9805 - val_loss: 0.0712 - val_accuracy: 0.9723\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9805 - val_loss: 0.0628 - val_accuracy: 0.9800\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0598 - accuracy: 0.9823 - val_loss: 0.0612 - val_accuracy: 0.9815\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 0.9815 - val_loss: 0.0631 - val_accuracy: 0.9815\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0584 - accuracy: 0.9820 - val_loss: 0.0609 - val_accuracy: 0.9815\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0573 - accuracy: 0.9828 - val_loss: 0.0597 - val_accuracy: 0.9815\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0571 - accuracy: 0.9826 - val_loss: 0.0593 - val_accuracy: 0.9815\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 0.9823 - val_loss: 0.0598 - val_accuracy: 0.9808\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9818 - val_loss: 0.0599 - val_accuracy: 0.9815\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9831 - val_loss: 0.0599 - val_accuracy: 0.9815\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9820 - val_loss: 0.0602 - val_accuracy: 0.9823\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9823 - val_loss: 0.0594 - val_accuracy: 0.9808\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0609 - val_accuracy: 0.9823\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0592 - val_accuracy: 0.9808\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9818 - val_loss: 0.0654 - val_accuracy: 0.9823\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0593 - val_accuracy: 0.9815\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0560 - accuracy: 0.9828 - val_loss: 0.0585 - val_accuracy: 0.9831\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0554 - accuracy: 0.9828 - val_loss: 0.0600 - val_accuracy: 0.9831\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0593 - val_accuracy: 0.9815\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0554 - accuracy: 0.9831 - val_loss: 0.0593 - val_accuracy: 0.9823\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0563 - accuracy: 0.9826 - val_loss: 0.0591 - val_accuracy: 0.9831\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0548 - accuracy: 0.9826 - val_loss: 0.0591 - val_accuracy: 0.9815\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0550 - accuracy: 0.9828 - val_loss: 0.0581 - val_accuracy: 0.9831\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.9836 - val_loss: 0.0587 - val_accuracy: 0.9823\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9823 - val_loss: 0.0634 - val_accuracy: 0.9808\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0568 - accuracy: 0.9836 - val_loss: 0.0585 - val_accuracy: 0.9823\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0550 - accuracy: 0.9831 - val_loss: 0.0581 - val_accuracy: 0.9823\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0598 - val_accuracy: 0.9823\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0559 - accuracy: 0.9846 - val_loss: 0.0598 - val_accuracy: 0.9831\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0576 - val_accuracy: 0.9831\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0590 - val_accuracy: 0.9831\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9826 - val_loss: 0.0579 - val_accuracy: 0.9815\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0542 - accuracy: 0.9838 - val_loss: 0.0564 - val_accuracy: 0.9831\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.9841 - val_loss: 0.0577 - val_accuracy: 0.9831\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0548 - accuracy: 0.9831 - val_loss: 0.0577 - val_accuracy: 0.9831\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.9841 - val_loss: 0.0676 - val_accuracy: 0.9723\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9815 - val_loss: 0.0604 - val_accuracy: 0.9815\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.9823 - val_loss: 0.0577 - val_accuracy: 0.9823\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.9790 - val_loss: 0.0573 - val_accuracy: 0.9838\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 0.9823 - val_loss: 0.0696 - val_accuracy: 0.9815\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9831 - val_loss: 0.0635 - val_accuracy: 0.9846\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.9800 - val_loss: 0.0576 - val_accuracy: 0.9838\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9810 - val_loss: 0.0586 - val_accuracy: 0.9831\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.9841 - val_loss: 0.0573 - val_accuracy: 0.9831\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.9843 - val_loss: 0.0589 - val_accuracy: 0.9831\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9820 - val_loss: 0.0648 - val_accuracy: 0.9823\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0562 - accuracy: 0.9831 - val_loss: 0.0575 - val_accuracy: 0.9831\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9833 - val_loss: 0.0570 - val_accuracy: 0.9838\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9849 - val_loss: 0.0600 - val_accuracy: 0.9808\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9820 - val_loss: 0.0566 - val_accuracy: 0.9831\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0647 - val_accuracy: 0.9838\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0610 - val_accuracy: 0.9854\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.9815 - val_loss: 0.0586 - val_accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "modelpath = './data/model/Ch14-4-bestmodel.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=2000, batch_size=500,\n",
    "                    validation_split=0.25, verbose=1,\n",
    "                    callbacks=[early_stopping_callback, checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 575us/step - loss: 0.0570 - accuracy: 0.9838\n",
      "Test accuracy: 0.983846127986908\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {score[1]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
